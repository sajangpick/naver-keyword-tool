"""
ë„¤ì´ë²„ ë‰´ìŠ¤ ìµœì‹  ì†Œì‹ ìë™ ìˆ˜ì§‘ ìŠ¤í¬ë¦½íŠ¸

ì‹¤í–‰ ì‹œ ì¹´í…Œê³ ë¦¬ë³„ë¡œ ë„¤ì´ë²„ ë‰´ìŠ¤ë¥¼ ê²€ìƒ‰í•´ Supabase news_board í…Œì´ë¸”ì— ì €ì¥í•©ë‹ˆë‹¤.
í™˜ê²½ ë³€ìˆ˜(.env)ì— ì•„ë˜ ê°’ë“¤ì´ ì„¤ì •ë˜ì–´ ìˆì–´ì•¼ í•©ë‹ˆë‹¤.
  - SUPABASE_URL
  - SUPABASE_SERVICE_KEY (ë˜ëŠ” SUPABASE_ANON_KEY)
  - NAVER_SEARCH_CLIENT_ID
  - NAVER_SEARCH_CLIENT_SECRET
"""

import os
import re
import time
from datetime import datetime
from typing import Dict, List, Optional
from urllib.parse import quote

import requests
from dotenv import load_dotenv
from supabase import Client, create_client

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ì˜ .env ë¡œë“œ
load_dotenv()
project_env = os.path.join(os.path.dirname(__file__), ".env")
if os.path.exists(project_env):
  load_dotenv(project_env)


# -----------------------------
# í™˜ê²½ ë³€ìˆ˜ ë° ìƒìˆ˜ ì„¤ì •
# -----------------------------
SUPABASE_URL = os.getenv("https://ptuzlubgggbgsophfcna.supabase.co")
SUPABASE_ANON_KEY = os.getenv("eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZSIsInJlZiI6InB0dXpsdWJnZ2diZ3NvcGhmY25hIiwicm9sZSI6ImFub24iLCJpYXQiOjE3NjA0MjEzMzQsImV4cCI6MjA3NTk5NzMzNH0.NaMMH7vVpcrFAi9IOQ0o_HF6rQ7dOdiAXAkxu6r84CE")
NAVER_CLIENT_ID = os.getenv("odP6PhMAXkzpFwTOtSj9")
NAVER_CLIENT_SECRET = os.getenv("OEWQdd3p6r")

MAX_PER_CATEGORY = int(os.getenv("NEWS_FETCH_LIMIT_PER_CATEGORY", "2"))
NAVER_RESULTS_PER_KEYWORD = int(os.getenv("NAVER_RESULTS_PER_KEYWORD", "5"))

# ë‰´ìŠ¤ ì›ë¬¸ ì¶”ì¶œ API ìš°ì„ ìˆœìœ„ (ENV ê°’ì„ ë¨¼ì € ì‚¬ìš©, ì—†ìœ¼ë©´ ê¸°ë³¸ê°’)
FETCH_BASE_CANDIDATES = [
  os.getenv("NEWS_FETCH_API_BASE"),
  os.getenv("NEWS_FETCH_FALLBACK_BASE"),
  "http://localhost:3003",
  "http://127.0.0.1:3003",
  "https://naver-keyword-tool.onrender.com",
]
FETCH_BASES = [base.rstrip("/") for base in FETCH_BASE_CANDIDATES if base]

# ì¹´í…Œê³ ë¦¬ë³„ ê¸°ë³¸ ê²€ìƒ‰ í‚¤ì›Œë“œ
CATEGORY_KEYWORDS: Dict[str, List[str]] = {
  "policy": [
    "ì™¸ì‹ì—… ì§€ì›ê¸ˆ",
    "ì†Œìƒê³µì¸ ì •ì±…",
    "ìŒì‹ì  ìœ„ìƒ ì ê²€",
  ],
  "trend": [
    "ì™¸ì‹ íŠ¸ë Œë“œ",
    "ë§›ì§‘ ì†Œë¹„ íŠ¸ë Œë“œ",
    "MZì„¸ëŒ€ ì‹ë‹¹ ì¸ê¸°",
  ],
  "management": [
    "ì‹ë‹¹ ê²½ì˜ ë…¸í•˜ìš°",
    "ì‹ë‹¹ ë§¤ì¶œ ì¦ê°€",
    "ìì˜ì—… ë§ˆì¼€íŒ… ì „ëµ",
  ],
  "ingredients": [
    "ì‹ìì¬ ê°€ê²© ë™í–¥",
    "ë†ì‚°ë¬¼ ê°€ê²© ìƒìŠ¹",
    "ìˆ˜ì‚°ë¬¼ ê°€ê²© ë³€ë™",
  ],
  "technology": [
    "ì‹ë‹¹ ê¸°ìˆ  ë„êµ¬",
    "ë°°ë‹¬ì•± ì—…ë°ì´íŠ¸",
    "POS ì‹œìŠ¤í…œ ë‰´ìŠ¤",
  ],
}


# -----------------------------
# ìœ í‹¸ í•¨ìˆ˜
# -----------------------------
def init_supabase() -> Optional[Client]:
  if not "https://ptuzlubgggbgsophfcna.supabase.co" or not "eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZSIsInJlZiI6InB0dXpsdWJnZ2diZ3NvcGhmY25hIiwicm9sZSI6ImFub24iLCJpYXQiOjE3NjA0MjEzMzQsImV4cCI6MjA3NTk5NzMzNH0.NaMMH7vVpcrFAi9IOQ0o_HF6rQ7dOdiAXAkxu6r84CE":
    print("âŒ Supabase í™˜ê²½ ë³€ìˆ˜ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. .env íŒŒì¼ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
    return None

  try:
    client = create_client( "https://ptuzlubgggbgsophfcna.supabase.co", "eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZSIsInJlZiI6InB0dXpsdWJnZ2diZ3NvcGhmY25hIiwicm9sZSI6ImFub24iLCJpYXQiOjE3NjA0MjEzMzQsImV4cCI6MjA3NTk5NzMzNH0.NaMMH7vVpcrFAi9IOQ0o_HF6rQ7dOdiAXAkxu6r84CE")
    return client
  except Exception as error:
    print(f"âŒ Supabase ì—°ê²° ì‹¤íŒ¨: {error}")
    return None


def clean_html(text: str) -> str:
  if not text:
    return ""
  cleaned = re.sub(r"<[^>]+>", " ", text)
  cleaned = re.sub(r"\s+", " ", cleaned)
  return cleaned.strip()


def fetch_naver_news(keyword: str) -> List[Dict]:
  if not NAVER_CLIENT_ID or not NAVER_CLIENT_SECRET:
    print("âš ï¸ ë„¤ì´ë²„ ê²€ìƒ‰ API í‚¤ê°€ ì—†ìŠµë‹ˆë‹¤. NAVER_SEARCH_CLIENT_ID / NAVER_SEARCH_CLIENT_SECRET ê°’ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
    return []

  url = "https://openapi.naver.com/v1/search/news.json"
  headers = {
    "X-Naver-Client-Id": NAVER_CLIENT_ID,
    "X-Naver-Client-Secret": NAVER_CLIENT_SECRET,
  }
  params = {
    "query": keyword,
    "display": NAVER_RESULTS_PER_KEYWORD,
    "sort": "date",
  }

  try:
    response = requests.get(url, headers=headers, params=params, timeout=5)
    response.raise_for_status()
    return response.json().get("items", [])
  except Exception as error:
    print(f"âŒ ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰ ì‹¤íŒ¨ ({keyword}): {error}")
    return []


def fetch_full_article(source_url: Optional[str]) -> Optional[Dict]:
  if not source_url:
    return None

  encoded = quote(source_url, safe="")
  for base in FETCH_BASES:
    fetch_url = f"{base}/api/news-fetch?url={encoded}"
    try:
      response = requests.get(fetch_url, timeout=12)
      if response.status_code == 200:
        data = response.json()
        if data.get("success") and data.get("data"):
          return data["data"]
      else:
        print(f"âš ï¸ ì›ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨ ({response.status_code}): {fetch_url}")
    except Exception as error:
      print(f"âš ï¸ ì›ë¬¸ ì¶”ì¶œ ì—ëŸ¬: {error} ({fetch_url})")
      continue

  return None


def build_news_payload(item: Dict, category: str) -> Optional[Dict]:
  title = clean_html(item.get("title", ""))
  summary = clean_html(item.get("description", ""))
  source_url = item.get("originallink") or item.get("link")

  if not title or not source_url:
    return None

  full_article = fetch_full_article(source_url)
  if full_article and full_article.get("content"):
    content_html = full_article["content"]
    final_source = full_article.get("sourceUrl") or source_url
  else:
    final_source = source_url
    if summary:
      content_html = f"<p>{summary}</p><p><br></p><p>ì¶œì²˜: <a href=\"{final_source}\" target=\"_blank\" rel=\"noopener\">{final_source}</a></p>"
    else:
      content_html = f"<p>ìì„¸í•œ ë‚´ìš©ì€ ì•„ë˜ ì›ë¬¸ ë§í¬ë¥¼ ì°¸ê³ í•´ì£¼ì„¸ìš”.</p><p><br></p><p>ì¶œì²˜: <a href=\"{final_source}\" target=\"_blank\" rel=\"noopener\">{final_source}</a></p>"

  payload = {
    "title": title[:255],
    "content": content_html,
    "category": category,
    "image_url": None,
    "source_url": final_source,
    "author": "NAVER_AUTO",
    "is_featured": False,
  }

  return payload


def already_exists(supabase_client: Client, source_urls: List[str]) -> List[str]:
  if not source_urls:
    return []

  try:
    result = (
      supabase_client
      .table("news_board")
      .select("source_url")
      .in_("source_url", source_urls)
      .execute()
    )
    if not result.data:
      return []

    return [row["source_url"] for row in result.data if row.get("source_url")]
  except Exception as error:
    print(f"âš ï¸ ê¸°ì¡´ ë‰´ìŠ¤ ì¡°íšŒ ì‹¤íŒ¨: {error}")
    return []


def insert_news(supabase_client: Client, items: List[Dict]) -> int:
  if not items:
    return 0

  try:
    supabase_client.table("news_board").insert(items).execute()
    return len(items)
  except Exception as error:
    print(f"âŒ ë‰´ìŠ¤ ì €ì¥ ì‹¤íŒ¨: {error}")
    return 0


# -----------------------------
# ë©”ì¸ ë¡œì§
# -----------------------------
def main() -> None:
  supabase_client = init_supabase()
  if not supabase_client:
    return

  collected: List[Dict] = []

  print("ğŸš€ ë„¤ì´ë²„ ë‰´ìŠ¤ ìë™ ì—…ë°ì´íŠ¸ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤.")
  started_at = time.perf_counter()

  for category, keywords in CATEGORY_KEYWORDS.items():
    print(f"\nğŸ—‚ ì¹´í…Œê³ ë¦¬ ì²˜ë¦¬ ì¤‘: {category}")
    category_items: List[Dict] = []

    for keyword in keywords:
      news_items = fetch_naver_news(keyword)
      for item in news_items:
        payload = build_news_payload(item, category)
        if payload:
          category_items.append(payload)
        if len(category_items) >= MAX_PER_CATEGORY:
          break
      if len(category_items) >= MAX_PER_CATEGORY:
        break
      time.sleep(0.4)  # í˜¸ì¶œ ê°„ ê°„ë‹¨í•œ ë”œë ˆì´

    print(f"  â†³ ìˆ˜ì§‘ëœ ê¸°ì‚¬ ìˆ˜: {len(category_items)}")
    collected.extend(category_items)

  dedupe_targets = [item["source_url"] for item in collected if item.get("source_url")]
  existing_sources = set(already_exists(supabase_client, dedupe_targets))

  new_items = [item for item in collected if item.get("source_url") not in existing_sources]

  if not new_items:
    print("\nâ„¹ï¸ ì‹ ê·œë¡œ ì¶”ê°€í•  ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤. (ëª¨ë‘ ì´ë¯¸ ë“±ë¡ë¨)")
    return

  inserted_count = insert_news(supabase_client, new_items)

  elapsed = time.perf_counter() - started_at
  now = datetime.now().strftime("%Y-%m-%d %H:%M:%S")

  if inserted_count > 0:
    print(f"\nâœ… {inserted_count}ê°œì˜ ë‰´ìŠ¤ë¥¼ ì €ì¥í–ˆìŠµë‹ˆë‹¤. ({now}, {elapsed:.1f}s)")
  else:
    print("\nâš ï¸ ë‰´ìŠ¤ë¥¼ ì €ì¥í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ë¡œê·¸ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")


if __name__ == "__main__":
  main()


