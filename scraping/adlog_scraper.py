"""
ADLOG.KR ë„¤ì´ë²„ í”Œë ˆì´ìŠ¤ ìˆœìœ„ ìŠ¤í¬ë˜í•‘ ìŠ¤í¬ë¦½íŠ¸
ë§¤ì¼ ìë™ìœ¼ë¡œ ìˆœìœ„ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•˜ì—¬ Supabaseì— ì €ì¥
"""

import requests
from bs4 import BeautifulSoup
import json
import os
from datetime import datetime
from dotenv import load_dotenv
import time
import pandas as pd

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

class AdlogScraper:
    def __init__(self):
        self.base_url = "https://m.place.naver.com/"
        self.adlog_url = "https://adlog.kr/adlog/naver_place_rank_check.php"
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'ko-KR,ko;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1'
        }
        
    def search_place_ranking(self, keyword, location=""):
        """
        íŠ¹ì • í‚¤ì›Œë“œë¡œ ë„¤ì´ë²„ í”Œë ˆì´ìŠ¤ ìˆœìœ„ ê²€ìƒ‰
        
        Args:
            keyword: ê²€ìƒ‰ í‚¤ì›Œë“œ (ì˜ˆ: "ì¹˜í‚¨", "ì¹´í˜")
            location: ì§€ì—­ (ì˜ˆ: "ê°•ë‚¨", "í™ëŒ€")
        
        Returns:
            ìˆœìœ„ ë°ì´í„° ë¦¬ìŠ¤íŠ¸
        """
        try:
            # ADLOG ê²€ìƒ‰ íŒŒë¼ë¯¸í„°
            search_query = f"{location} {keyword}" if location else keyword
            
            params = {
                'keyword': search_query,
                'type': 'place'  # í”Œë ˆì´ìŠ¤ ê²€ìƒ‰
            }
            
            print(f"ğŸ” ê²€ìƒ‰ì¤‘: {search_query}")
            
            # ADLOG API í˜¸ì¶œ (ì‹¤ì œ URLê³¼ íŒŒë¼ë¯¸í„°ëŠ” ì‚¬ì´íŠ¸ ë¶„ì„ í›„ ìˆ˜ì • í•„ìš”)
            response = requests.get(
                self.adlog_url,
                params=params,
                headers=self.headers,
                timeout=10
            )
            
            if response.status_code == 200:
                soup = BeautifulSoup(response.text, 'html.parser')
                
                # ìˆœìœ„ ë°ì´í„° íŒŒì‹± (ì‹¤ì œ HTML êµ¬ì¡°ì— ë§ê²Œ ìˆ˜ì • í•„ìš”)
                rankings = []
                
                # í…Œì´ë¸” ì°¾ê¸° (ADLOG ì‹¤ì œ êµ¬ì¡°ì— ë§ê²Œ ìˆ˜ì •)
                ranking_table = soup.find('table', class_='ranking-table')
                if not ranking_table:
                    ranking_table = soup.find('table')  # í´ë˜ìŠ¤ëª… ì—†ì„ ê²½ìš°
                
                if ranking_table:
                    rows = ranking_table.find_all('tr')[1:]  # í—¤ë” ì œì™¸
                    
                    for idx, row in enumerate(rows[:20], 1):  # ìƒìœ„ 20ê°œë§Œ
                        cols = row.find_all('td')
                        if len(cols) >= 3:
                            ranking_data = {
                                'rank': idx,
                                'place_name': cols[1].get_text(strip=True),
                                'place_id': cols[0].get_text(strip=True),  # í”Œë ˆì´ìŠ¤ ID
                                'category': cols[2].get_text(strip=True) if len(cols) > 2 else '',
                                'keyword': search_query,
                                'search_date': datetime.now().strftime('%Y-%m-%d'),
                                'search_time': datetime.now().strftime('%H:%M:%S')
                            }
                            rankings.append(ranking_data)
                            print(f"  {idx}ìœ„: {ranking_data['place_name']}")
                
                return rankings
            else:
                print(f"âŒ ìš”ì²­ ì‹¤íŒ¨: {response.status_code}")
                return []
                
        except Exception as e:
            print(f"âŒ ìŠ¤í¬ë˜í•‘ ì˜¤ë¥˜: {str(e)}")
            return []
    
    def save_to_json(self, data, filename=None):
        """
        ë°ì´í„°ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥
        """
        if not filename:
            filename = f"adlog_ranking_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        
        filepath = f"scraping/data/{filename}"
        os.makedirs(os.path.dirname(filepath), exist_ok=True)
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… ì €ì¥ ì™„ë£Œ: {filepath}")
        return filepath
    
    def save_to_csv(self, data, filename=None):
        """
        ë°ì´í„°ë¥¼ CSV íŒŒì¼ë¡œ ì €ì¥
        """
        if not filename:
            filename = f"adlog_ranking_{datetime.now().strftime('%Y%m%d')}.csv"
        
        filepath = f"scraping/data/{filename}"
        os.makedirs(os.path.dirname(filepath), exist_ok=True)
        
        df = pd.DataFrame(data)
        df.to_csv(filepath, index=False, encoding='utf-8-sig')
        
        print(f"âœ… CSV ì €ì¥ ì™„ë£Œ: {filepath}")
        return filepath
    
    def track_multiple_keywords(self, keywords_list):
        """
        ì—¬ëŸ¬ í‚¤ì›Œë“œì˜ ìˆœìœ„ë¥¼ í•œë²ˆì— ì¶”ì 
        
        Args:
            keywords_list: í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸
            ì˜ˆ: [
                {'keyword': 'ì¹˜í‚¨', 'location': 'ê°•ë‚¨'},
                {'keyword': 'ì¹´í˜', 'location': 'í™ëŒ€'},
                {'keyword': 'í•œì‹', 'location': 'ì„œì´ˆ'}
            ]
        """
        all_rankings = []
        
        for item in keywords_list:
            keyword = item.get('keyword', '')
            location = item.get('location', '')
            
            # ê° í‚¤ì›Œë“œ ê²€ìƒ‰
            rankings = self.search_place_ranking(keyword, location)
            all_rankings.extend(rankings)
            
            # API ë¶€í•˜ ë°©ì§€ë¥¼ ìœ„í•œ ë”œë ˆì´
            time.sleep(2)
        
        return all_rankings
    
    def find_my_restaurant(self, restaurant_name, rankings):
        """
        ë‚´ ì‹ë‹¹ì˜ ìˆœìœ„ ì°¾ê¸°
        
        Args:
            restaurant_name: ë‚´ ì‹ë‹¹ ì´ë¦„
            rankings: ì „ì²´ ìˆœìœ„ ë°ì´í„°
        
        Returns:
            ë‚´ ì‹ë‹¹ì˜ ìˆœìœ„ ì •ë³´
        """
        my_rankings = []
        
        for ranking in rankings:
            if restaurant_name in ranking['place_name']:
                my_rankings.append({
                    'keyword': ranking['keyword'],
                    'rank': ranking['rank'],
                    'date': ranking['search_date']
                })
                print(f"ğŸ¯ '{ranking['keyword']}' ê²€ìƒ‰ ì‹œ {ranking['rank']}ìœ„!")
        
        if not my_rankings:
            print(f"ğŸ˜¢ '{restaurant_name}'ì„(ë¥¼) ìˆœìœ„ì—ì„œ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        return my_rankings


# ì‚¬ìš© ì˜ˆì œ
if __name__ == "__main__":
    # ìŠ¤í¬ë˜í¼ ì´ˆê¸°í™”
    scraper = AdlogScraper()
    
    # 1. ë‹¨ì¼ í‚¤ì›Œë“œ ê²€ìƒ‰
    print("=" * 50)
    print("1ï¸âƒ£ ë‹¨ì¼ í‚¤ì›Œë“œ ê²€ìƒ‰")
    print("=" * 50)
    rankings = scraper.search_place_ranking("ì¹˜í‚¨", "ê°•ë‚¨")
    
    # 2. ì—¬ëŸ¬ í‚¤ì›Œë“œ ì¶”ì 
    print("\n" + "=" * 50)
    print("2ï¸âƒ£ ì—¬ëŸ¬ í‚¤ì›Œë“œ ì¶”ì ")
    print("=" * 50)
    keywords_to_track = [
        {'keyword': 'ì¹˜í‚¨', 'location': 'ê°•ë‚¨'},
        {'keyword': 'ì¹´í˜', 'location': 'í™ëŒ€'},
        {'keyword': 'í•œì‹', 'location': 'ì„œì´ˆ'}
    ]
    all_rankings = scraper.track_multiple_keywords(keywords_to_track)
    
    # 3. ë°ì´í„° ì €ì¥
    if all_rankings:
        scraper.save_to_json(all_rankings)
        scraper.save_to_csv(all_rankings)
    
    # 4. ë‚´ ì‹ë‹¹ ìˆœìœ„ ì°¾ê¸°
    print("\n" + "=" * 50)
    print("3ï¸âƒ£ ë‚´ ì‹ë‹¹ ìˆœìœ„ ì°¾ê¸°")
    print("=" * 50)
    my_restaurant = "BBQì¹˜í‚¨"  # ì˜ˆì‹œ
    my_ranks = scraper.find_my_restaurant(my_restaurant, all_rankings)
    
    # 5. ê²°ê³¼ ìš”ì•½
    print("\n" + "=" * 50)
    print("ğŸ“Š ì˜¤ëŠ˜ì˜ ìˆœìœ„ ìš”ì•½")
    print("=" * 50)
    if my_ranks:
        for rank_info in my_ranks:
            print(f"  â€¢ {rank_info['keyword']}: {rank_info['rank']}ìœ„")
    
    print(f"\nâœ… ì´ {len(all_rankings)}ê°œ ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ!")
